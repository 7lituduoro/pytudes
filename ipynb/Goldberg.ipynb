{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Unreasonable Effectiveness of Character-level Language Models\n",
    "# (and why RNNs are still cool)\n",
    "\n",
    "## By [Yoav Goldberg](http://www.cs.biu.ac.il/~yogo) (2015)\n",
    "\n",
    "#### (with minor changes by Peter Norvig (2022) for modern Python 3)\n",
    "\n",
    "<hr>\n",
    "\n",
    "RNNs, LSTMs and Deep Learning are all the rage, and a recent [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) by Andrej Karpathy is doing a great job explaining what these models are and how to train them.\n",
    "It also provides some very impressive results of what they are capable of.  This is a great post, and if you are interested in natural language, machine learning or neural networks you should definitely read it. \n",
    "\n",
    "Go [**read it now**](http://karpathy.github.io/2015/05/21/rnn-effectiveness/), then come back here. \n",
    "\n",
    "You're back? good. Impressive stuff, huh? How could the network learn to imitate the input like that?\n",
    "Indeed. I was quite impressed as well.\n",
    "\n",
    "However, it feels to me that most readers of the post are impressed by the wrong reasons.\n",
    "This is because they are not familiar with **unsmoothed maximum-liklihood character level language models** and their unreasonable effectiveness at generating rather convincing natural language outputs.\n",
    "\n",
    "In what follows I will briefly describe these character-level maximum-likelihood langauge models, which are much less magical than RNNs and LSTMs, and show that they too can produce a rather convincing Shakespearean prose. I will also show about 30 lines of python code that take care of both training the model and generating the output. Compared to this baseline, the RNNs may seem somehwat less impressive. So why was I impressed? I will explain this too, below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsmoothed Maximum Likelihood Character Level Language Model \n",
    "\n",
    "The name is quite long, but the idea is very simple.  We want a model whose job is to guess the next character based on the previous *n* letters. For example, having seen `ello`, the next characer is likely to be either a commma or space (if we assume is is the end of the word \"hello\"), or the letter `w` if we believe we are in the middle of the word \"mellow\". Humans are quite good at this, but of course seeing a larger history makes things easier (if we were to see 5 letters instead of 4, the choice between space and `w` would have been much easier).\n",
    "\n",
    "We will call *n*, the number of letters we need to guess based on, the _order_ of the language model.\n",
    "\n",
    "RNNs and LSTMs can potentially learn infinite-order language model (they guess the next character based on a \"state\" which supposedly encode all the previous history). We here will restrict ourselves to a fixed-order language model.\n",
    "\n",
    "So, we are seeing *n* letters, and need to guess the *n+1*th one. We are also given a large-ish amount of text (say, all of Shakespear works) that we can use. How would we go about solving this task?\n",
    "\n",
    "Mathematiacally, we would like to learn a function *P(c* | *h)*. Here, *c* is a character, *h* is a *n*-letters history, and *P(c* | *h)* stands for how likely is it to see *c* after we've seen *h*.\n",
    "\n",
    "Perhaps the simplest approach would be to just count and divide (a.k.a **maximum likelihood estimates**). We will count the number of times each letter *c* appeared after *h*, and divide by the total numbers of letters appearing after *h*. The **unsmoothed** part means that if we did not see a given letter following *h*, we will just give it a probability of zero.\n",
    "\n",
    "And that's all there is to it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Code\n",
    "\n",
    "Here is the code for training the model. `fname` is a file to read the characters from. `order` is the history size to consult. Note that we pad the data with `order` leading characters so that we also learn how to start.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Tuple\n",
    "\n",
    "class LanguageModel(defaultdict):\n",
    "    \"\"\"A mapping from `order` history characters to a list of ('c', probability) pairs,\n",
    "    e.g., for order=4, {'spea': [('k', 0.99), ('r', 0.01)])}.\"\"\"\n",
    "    def __init__(self, order): self.order = order\n",
    "\n",
    "def train_char_lm(fname, order=4) -> LanguageModel:\n",
    "    \"\"\"Train an `order`-gram character-level language model on all the text in `fname`.\"\"\"\n",
    "    lm = LanguageModel(order)\n",
    "    data = (PAD * order) + open(fname).read()\n",
    "    # First read data into Counters of characters; then normalize\n",
    "    lm.default_factory = Counter \n",
    "    for i in range(order, len(data)):\n",
    "        history, char = data[i - order:i], data[i]\n",
    "        lm[history][char] += 1\n",
    "    for history in lm:\n",
    "        lm[history] = normalize(lm[history])\n",
    "    return lm\n",
    "\n",
    "def normalize(counter) -> List[Tuple[str, float]]:\n",
    "    \"\"\"Return (key, val) pairs, normalized so values sum to 1.0, largest first.\"\"\"\n",
    "    total = float(sum(counter.values()))\n",
    "    return [(k, v / total) for k, v in counter.most_common()]\n",
    "\n",
    "PAD = '`' # Character to pad the beginning of a text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train it on Andrej's Shakespeare text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  167204  832301 4573338 shakespeare_input.txt\n"
     ]
    }
   ],
   "source": [
    "! [ -f shakespeare_input.txt ] || curl -O https://norvig.com/ngrams/shakespeare_input.txt\n",
    "! wc shakespeare_input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = train_char_lm(\"shakespeare_input.txt\", order=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok. Now let's do some queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('w', 0.817717206132879),\n",
       " ('r', 0.059625212947189095),\n",
       " ('u', 0.03747870528109029),\n",
       " (',', 0.027257240204429302),\n",
       " (\"'\", 0.017035775127768313),\n",
       " (' ', 0.013628620102214651),\n",
       " ('.', 0.0068143100511073255),\n",
       " ('?', 0.0068143100511073255),\n",
       " ('!', 0.0068143100511073255),\n",
       " (':', 0.005110732538330494),\n",
       " ('n', 0.0017035775127768314)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm['ello']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('t', 1.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm['Firs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('S', 0.16292134831460675),\n",
       " ('L', 0.10674157303370786),\n",
       " ('C', 0.09550561797752809),\n",
       " ('G', 0.0898876404494382),\n",
       " ('M', 0.0593900481540931),\n",
       " ('t', 0.05377207062600321),\n",
       " ('W', 0.033707865168539325),\n",
       " ('s', 0.03290529695024077),\n",
       " ('o', 0.030497592295345103),\n",
       " ('b', 0.024879614767255216),\n",
       " ('w', 0.024077046548956663),\n",
       " ('a', 0.02247191011235955),\n",
       " ('m', 0.02247191011235955),\n",
       " ('n', 0.020064205457463884),\n",
       " ('h', 0.019261637239165328),\n",
       " ('O', 0.018459069020866775),\n",
       " ('i', 0.016853932584269662),\n",
       " ('d', 0.015248796147672551),\n",
       " ('P', 0.014446227929373997),\n",
       " ('c', 0.012841091492776886),\n",
       " ('F', 0.012038523274478331),\n",
       " ('f', 0.011235955056179775),\n",
       " ('g', 0.011235955056179775),\n",
       " ('l', 0.01043338683788122),\n",
       " ('I', 0.009630818619582664),\n",
       " ('B', 0.009630818619582664),\n",
       " ('p', 0.00882825040128411),\n",
       " ('K', 0.008025682182985553),\n",
       " ('r', 0.0072231139646869984),\n",
       " ('A', 0.0056179775280898875),\n",
       " ('H', 0.0040128410914927765),\n",
       " ('k', 0.0040128410914927765),\n",
       " ('e', 0.0032102728731942215),\n",
       " ('T', 0.0032102728731942215),\n",
       " ('D', 0.0032102728731942215),\n",
       " ('y', 0.002407704654895666),\n",
       " ('v', 0.002407704654895666),\n",
       " ('u', 0.0016051364365971107),\n",
       " ('q', 0.0016051364365971107),\n",
       " ('E', 0.0016051364365971107),\n",
       " ('R', 0.0008025682182985554),\n",
       " ('N', 0.0008025682182985554),\n",
       " (\"'\", 0.0008025682182985554)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm['rst ']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So `\"ello\"` is followed by either space, punctuation or `w` (or `r`, `u`, `n`), `\"Firs\"` is pretty much deterministic, and the word following `\"rst \"` can start with pretty much every letter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating from the model\n",
    "\n",
    "Generating is also very simple. To generate a letter, we will take the history, look at the last *order* characters, and then sample a random letter based on the corresponding distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_character(lm, history) -> str:\n",
    "    \"\"\"Given a history of characters, sample a random next character from `lm`.\"\"\"\n",
    "    p = random.random()\n",
    "    for c, v in lm[history]:\n",
    "        if p <= v: \n",
    "            return c\n",
    "        p -= v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate a passage of *k* characters, we just seed it with the initial history and run letter generation in a loop, updating the history at each turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lm, length=1000) -> str:\n",
    "    \"\"\"Sample a random `length`-long passage from `lm`.\"\"\"\n",
    "    history = PAD * lm.order\n",
    "    out = []\n",
    "    for i in range(length):\n",
    "        c = generate_character(lm, history)\n",
    "        history = history[1:] + c\n",
    "        out.append(c)\n",
    "    return ''.join(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated Shakespeare from different order models\n",
    "\n",
    "Let's try to generate text based on different language-model orders. Let's start with something silly:\n",
    "\n",
    "## order 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fiestis the so, ing'd? was hathy now by ollood re hichaved.\n",
      "\n",
      "MANCE:\n",
      "So oul and carried bear wilese com now ifeck.\n",
      "\n",
      "Vere ame\n",
      "A Cad prover, toods\n",
      "Whiseto wousave.\n",
      "\n",
      "But to prath troureak do thque.\n",
      "\n",
      "SUS:\n",
      "Up to light sonste cat ing ing.\n",
      "\n",
      "EXASTHASTARVENRAY:\n",
      "In Clee rah, th sher gollooke herponefted? hin;\n",
      "Yout the cat this nothaved; tord belf.\n",
      "Gody hat threit es faw'd you wer:\n",
      "If ailt how at be it:\n",
      "A to miden lanign day himemalo; withat thy shat ans, th al at's th for tent! EDGAN:\n",
      "As livill your frows;\n",
      "Ay grour Romer, ant cou, dign,\n",
      "Behould a fieved theek\n",
      "I wood by ne untle, blove;\n",
      "O' womit hat I lass an.\n",
      "\n",
      "Tword nothee, plasou? To fer'd thisichou some me,\n",
      "Mill rol.\n",
      "\n",
      "Ha! I deast ind the many ouldou not: seep amend at's of to deat hent: whis dick.\n",
      "\n",
      "KINERMISATEUS Let a reempat ung.\n",
      "Mus he maltie. WAR Con ound our wilivem I;\n",
      "As rom de she a weat pey prews say arn Cithe ming paseas duch and such chat your you go my Duke ens;\n",
      "EDWARUTUS:\n",
      "I lontassues is se day Ladand dre lin wass re becy, ton\n",
      "Theas \n"
     ]
    }
   ],
   "source": [
    "lm = train_char_lm(\"shakespeare_input.txt\", order=2)\n",
    "print(generate_text(lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## order 4 \n",
    "\n",
    "Order 2 was not so great... but what if we increase the order to 4?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Outland? Look upon it is his better offence, which I at out:\n",
      "Sir John.\n",
      "\n",
      "KING PHILIP:\n",
      "Why yawn companiel?\n",
      "\n",
      "CAMILLO:\n",
      "\n",
      "COSTARD:\n",
      "No, safe to my utter?\n",
      "\n",
      "APEMANTUS:\n",
      "Nor I worship, give thee: I'll draws eart sociable, noble greaten this lease are could hide use meet, it is trued Brutus me against would burn some to him the grace no more thy play.\n",
      "\n",
      "VIOLA:\n",
      "You affection,\n",
      "Therein men\n",
      "such on from that I my grind homel, call, were beseech you evenger.\n",
      "\n",
      "PISTOL:\n",
      "Let us should blow\n",
      "Will be to remembrancis!\n",
      "\n",
      "MACBETH:\n",
      "\n",
      "MARGARET:\n",
      "The to unworthwith treath,\n",
      "Drop of tune's my bloody. What will me thy Fame is attaining her bed, and sir; he us all give some young Rome,\n",
      "Hostess Shortly come any rascal, o'er them off this Englands this be an assius, aris. Pray you saw them she day opinion for at is my he dange the fight, sir, her, and so long bound thee.\n",
      "\n",
      "DESDEMONA:\n",
      "Can I have foot the violets! where's amore you these\n",
      "him in hold fairs speak no pays and father?\n",
      "'Tis burn after the air dought amissive,\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lm = train_char_lm(\"shakespeare_input.txt\", order=4)\n",
    "print(generate_text(lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## order 7\n",
    "\n",
    "Order 4 is already quite reasonable, and reads like English. Just 4 letters history! What if we increase it to 7?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Ay, but I therefore let him from shore, to Mercury, set: then, some comfort.\n",
      "\n",
      "COUNTESS:\n",
      "Tell her, indeed, too, good show\n",
      "Can heavens, how I came hither art the next.\n",
      "\n",
      "BARDOLPH:\n",
      "By these men.\n",
      "But direction: and, for secrecy:\n",
      "We shall I be,\n",
      "That so with your uncle to couples with her beauty is bound as thought! an 'twere the occasion this to-morrow night;\n",
      "Unless your mouths, if the realm.\n",
      "I never enter'd Pucelle jointure.\n",
      "\n",
      "SIMPCOX:\n",
      "God keep from whom I, indeed too, 'mong other both at our fair pillow for a\n",
      "man against the prince, there!\n",
      "\n",
      "MARIA:\n",
      "You shall deliver us from their own grace, pardon you: yet Count Comfect; an the year groan at it.\n",
      "\n",
      "SIR ANDREW:\n",
      "'Slight, than you can be!\n",
      "Through thou scurvy railing, may surrender; so must be, love, kill myself, thou wert born i' the people,\n",
      "You are\n",
      "going to my father's show thyself?\n",
      "\n",
      "MONTAGUE:\n",
      "O, when nobles should you in.\n",
      "\n",
      "TRINCULO:\n",
      "Excellent this sleep,\n",
      "And say 'God save you to-morrow morning, but what you fear?  myself in such \n"
     ]
    }
   ],
   "source": [
    "lm = train_char_lm(\"shakespeare_input.txt\", order=7)\n",
    "print(generate_text(lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How about order 10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "O royal Caesar!\n",
      "\n",
      "First Tribune:\n",
      "We will make amends now: get you gone.\n",
      "\n",
      "MARTIUS:\n",
      "What else, fellow? Pray you, let me go.\n",
      "But what, is he arrested? Tell me at whose burthen\n",
      "The anger'd any heart alive\n",
      "To hear meekly, sir, they shall know it, would much better used\n",
      "On Navarre and his brethren come?\n",
      "\n",
      "BUCKINGHAM:\n",
      "Give me any gage of this seeming.\n",
      "\n",
      "HORATIO:\n",
      "Ay, my good lord: our time too brief: I will thither: gracious offers from the humble-bees,\n",
      "And let another general, thou shouldst not bear my standard of the wheat must needs\n",
      "Appear unkinglike.\n",
      "\n",
      "CAIUS LUCIUS:\n",
      "I have, my lord,\n",
      "I should not; for he this\n",
      "very day receive it friendly; but from this time.\n",
      "\n",
      "VALENTINE:\n",
      "How use doth breed a habit in a man!\n",
      "This shadow\n",
      "Doth limp behind that doth warrant.\n",
      "Hark, how our steeds for present business, nor my power\n",
      "To o'erthrown Antony,\n",
      "And very weak and melancholy upon your stubborn ancient skill to fear and cold hand of death hath snatch'd that it us befitted\n",
      "To bear themselves made, \n"
     ]
    }
   ],
   "source": [
    "lm = train_char_lm(\"shakespeare_input.txt\", order=10)\n",
    "print(generate_text(lm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This works pretty well\n",
    "\n",
    "With an order of 4, we already get quite reasonable results. Increasing the order to 7 (about a word and a half of history) or 10 (about two short words of history) already gets us quite passable Shakepearan text. I'd say it is on par with the examples in Andrej's post. And how simple and un-mystical the model is!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## So why am I impressed with the RNNs after all?\n",
    "\n",
    "Generating English a character at a time -- not so impressive in my view. The RNN needs to learn the previous *n* letters, for a rather small *n*, and that's it. \n",
    "\n",
    "However, Karpathy's C++ code generation example is very impressive. Why? because of the context awareness. Note that in all of Karpathy's posted examples, the code is well indented, the braces and brackets are correctly nested, and even the comments start and end correctly. This is not something that can be achieved by simply looking at the previous *n* characters. \n",
    "\n",
    "If Karpathy's examples are not cherry-picked, and the output is generally that nice, then the LSTM did learn something not trivial at all.\n",
    "\n",
    "# Linux Kernel C++ Code\n",
    "\n",
    "Just for the fun of it, let's see what our simple language model does with the Linux-kernel code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  241465  759639 6206997 linux_input.txt\n"
     ]
    }
   ],
   "source": [
    "! [ -f linux_input.txt ] || curl -O https://norvig.com/ngrams/linux_input.txt\n",
    "! wc linux_input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*\n",
      " * linux/kernel/printk.c\n",
      " *\n",
      " *  Copyright (C) 1992, 1998-2004 Linus Torvalds, Ingo Molnar <mingo@redhat.com>\n",
      " * Copyright (c) 2009 Rafael J. Wysocki <rjw@sisk.pl>, Novell Inc.\n",
      " *\n",
      " * This overall must be zero */\n",
      "\ttxc->ppsfreq, &utp->freq) ||\n",
      "\t\t\t__get_user(handler, &act->sa_handler;\n",
      "\t\tnext_event.tv64 != KTIME_MAX;\n",
      "\tnext_event = parent_freezer(freezer))) {\n",
      "\t\tif (!access_ok(VERIFY_READ, u_event, sizeof(*src);\n",
      "\n",
      "\t/* Convert (if necessary to check that the target CPU.\n",
      " */\n",
      "void gcov_info *info)\n",
      "{\n",
      "\treturn (copied == sizeof(debug_alloc_header *)\n",
      "\t\t\t\t(debug_alloc_header {\n",
      "\tchar reserved fields\\n\");\n",
      "\t\treturn;\n",
      "\n",
      "\tperf_output_begin(&handle, &snapshot_data *data)\n",
      "{\n",
      "\tstruct mcs_spinlock */\n",
      "void gcov_info *get_accumulated_info(node, info);\n",
      "\tif (i > len)\n",
      "\t\tcnt = TRACE_SIGNAL_DELIVERED;\n",
      "out:\n",
      "\ttracing_stop_tr(tr);\n",
      "\n",
      "\t__trace_function_single(int cpu);\n",
      "\n",
      "extern void kdb_dumpregs(regs);\n",
      "\t\tdbg_activate_work(work);\n",
      "\n",
      "\t/*\n",
      "\t * Can't set/change the\"\n",
      "\t\t\t\t\t   \"1\", enable);\n",
      "extern struct ftrace_graph_entry_leaf(struc\n"
     ]
    }
   ],
   "source": [
    "lm = train_char_lm(\"linux_input.txt\", order=10)\n",
    "print(generate_text(lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*\n",
      " * linux/kernel/time/tick-broadcast-hrtimer.c\n",
      " * This file emulates a local clock event device cannot go away as\n",
      "\t * long as we hold\n",
      "\t * lock->wait_lock held.\n",
      " */\n",
      "void __ptrace_unlink - unlink/remove profiling data set with an existing node. Needs to be called with lock->wait_lock);\n",
      "\t *\t\t\t\t\tacquire(lock);\n",
      "\t * or:\n",
      "\t *\n",
      "\t * unlock(wait_lock);\n",
      "\t *\t\t\t\t\tacquire(lock);\n",
      "\t */\n",
      "\treturn rc;\n",
      "}\n",
      "\n",
      "static int get_clock_desc(id, &cd);\n",
      "\tif (err)\n",
      "\t\treturn err;\n",
      "\n",
      "\tif (cd.clk->ops.clock_adjtime(clockid_t id, struct timex __user *) &txc);\n",
      "\tset_fs(oldfs);\n",
      "\tif (!err && compat_put_timespec(&out, rmtp))\n",
      "\t\treturn -EFAULT;\n",
      "\t}\n",
      "\tforce_successful_syscall_return();\n",
      "\treturn compat_jiffies_to_clock_t);\n",
      "\n",
      "u64 nsec_to_clock_t(tsk->delays->blkio_start = ktime_get();\n",
      "\tif (!ret) {\n",
      "\t\tprintk(KERN_CONT \".. corrupted trace buffer .. \");\n",
      "\treturn -1;\n",
      "}\n",
      "\n",
      "/* Will lock the rq it finds */\n",
      "static struct cgroup_subsys *ss;\n",
      "\tchar *tok;\n",
      "\tint ssid, ret;\n",
      "\n",
      "\t/* Do not accept '\\n' to prevent making /proc/<pid>/cgroup.\n",
      " */\n",
      "int zap_other_thread\n"
     ]
    }
   ],
   "source": [
    "lm = train_char_lm(\"linux_input.txt\", order=15)\n",
    "print(generate_text(lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*\n",
      " * linux/kernel/irq/handle.c\n",
      " *\n",
      " * Copyright (C) 2003-2004 Amit S. Kale <amitkale@linsyssoft.com>\n",
      " * Copyright (C) 2008 Steven Rostedt <srostedt@redhat.com>\n",
      " *\n",
      " */\n",
      "#include <linux/irq.h>\n",
      "#include <linux/notifier.h>\n",
      "#include <linux/init.h>\n",
      "#include <linux/vmalloc.h>\n",
      "#include <asm/sections.h>\n",
      "\n",
      "#include <crypto/hash.h>\n",
      "#include <keys/asymmetric-type.h>\n",
      "#include <keys/system_keyring.h>\n",
      "#include \"module-internal.h\"\n",
      "\n",
      "struct key *system_trusted_keyring, 1),\n",
      "\t\t\t\t\t   \"asymmetric\",\n",
      "\t\t\t\t\t   NULL,\n",
      "\t\t\t\t\t   p,\n",
      "\t\t\t\t\t   plen,\n",
      "\t\t\t\t\t   ((KEY_POS_ALL & ~KEY_POS_SETATTR) |\n",
      "\t\t\t      KEY_USR_VIEW | KEY_USR_READ),\n",
      "\t\t\t\t\t   KEY_ALLOC_NOT_IN_QUOTA |\n",
      "\t\t\t\t\t   KEY_ALLOC_TRUSTED);\n",
      "\t\tif (IS_ERR(key)) {\n",
      "\t\tswitch (PTR_ERR(key)) {\n",
      "\t\t\t/* Hide some search errors */\n",
      "\t\tcase -EACCES:\n",
      "\t\tcase -ENOTDIR:\n",
      "\t\tcase -EAGAIN:\n",
      "\t\t\treturn ERR_PTR(-EACCES);\n",
      "\n",
      "\t\t/*\n",
      "\t\t * We could be clever and allow to attach a event to an\n",
      "\t\t * offline CPU and activate it when the CPU comes up, but\n",
      "\t\t * that's for later.\n",
      "\t\t */\n",
      "\t\tif (!cpu_online(cpu))\n",
      "\t\tc\n"
     ]
    }
   ],
   "source": [
    "lm = train_char_lm(\"linux_input.txt\", order=20)\n",
    "print(generate_text(lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*\n",
      " * linux/kernel/itimer.c\n",
      " *\n",
      " * Copyright (C) 2010\t\tSUSE Linux Products GmbH\n",
      " * Copyright (C) 2002 2003 by MontaVista Software.\n",
      " *\n",
      " * 2004-06-01  Fix CLOCK_REALTIME clock/timer TIMER_ABSTIME bug.\n",
      " *\t\t\t     Copyright (C) 2004-2006 Ingo Molnar\n",
      " *  Copyright (C) 2004 Nadia Yvette Chambers, IBM\n",
      " * (C) 2004 Nadia Yvette Chambers\n",
      " */\n",
      "#include <linux/cred.h>\n",
      "#include <linux/module.h>\n",
      "#include <linux/mutex.h>\n",
      "#include <linux/mutex.h>\n",
      "#include <linux/securebits.h>\n",
      "#include <linux/clocksource.h>\n",
      "#include <linux/timecounter.h>\n",
      "\n",
      "void timecounter_init(struct timecounter *tc)\n",
      "{\n",
      "\tcycle_t cycle_now, cycle_delta;\n",
      "\n",
      "\tsleeptime_injected = true;\n",
      "\t} else if (timespec64_compare(&ts_new, &timekeeping_suspend_time) > 0) {\n",
      "\t\tts_delta = timespec64_sub(tk_xtime(tk), timekeeping_suspend_time, delta_delta);\n",
      "\t\t}\n",
      "\t}\n",
      "\n",
      "\ttimekeeping_update(tk, TK_MIRROR | TK_CLOCK_WAS_SET);\n",
      "\n",
      "\tif (action & TK_MIRROR)\n",
      "\t\tmemcpy(&shadow_timekeeper, &tk_core.timekeeper;\n",
      "\tstruct clocksource *clock = tk->tkr_mono.clock;\n",
      "\ttk->tkr_mono.read = \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/*\n",
      " * linux/kernel/irq/handle.c\n",
      " *\n",
      " * Copyright (C) 2000-2001 VERITAS Software Corporation.\n",
      " * Copyright (C) 2011 Peter Zijlstra <pzijlstr@redhat.com>\n",
      " *  Copyright (C) 2004-2006 Tom Rini <trini@kernel.crashing.org>\n",
      " * Copyright (C) 2005-2006, Thomas Gleixner, Russell King\n",
      " *\n",
      " * This file contains the /proc/irq/ handling code.\n",
      " */\n",
      "\n",
      "#include <linux/percpu.h>\n",
      "#include <linux/cpuset.h>\n",
      "#include <linux/uaccess.h>\n",
      "#include <linux/slab.h>\n",
      "#include <linux/file.h>\n",
      "#include <linux/export.h>\n",
      "#include <linux/signal.h>\n",
      "#include <linux/debug_locks.h>\n",
      "#include <linux/seq_file.h>\n",
      "#include <linux/uaccess.h>\n",
      "\n",
      "#include <trace/events/timer.h>\n",
      "\n",
      "/*\n",
      " * Per cpu nohz control structure\n",
      " */\n",
      "static DEFINE_PER_CPU(struct task_struct *p;\n",
      "\tint retval;\n",
      "\n",
      "\trcu_read_lock();\n",
      "\tfor_each_domain(cpu, sd)\n",
      "\t\tdomain_num++;\n",
      "\tentry = table = sd_alloc_ctl_entry(domain_num + 1);\n",
      "\tif (table == NULL)\n",
      "\t\treturn NULL;\n",
      "\n",
      "\t/*\n",
      "\t * We repeat when a time extend is encountered or we hit\n",
      "\t * the end of the page to save the\n",
      "\t\t * missed events, then record it there.\n",
      "\t\t */\n",
      "\t\tif (BUF_PAGE_SIZE - commit);\n",
      "\n",
      " out_unlock:\n",
      "\traw_spin_unlock_irqrestore(&nh->lock, flags);\n",
      "\treturn ret;\n",
      "}\n",
      "EXPORT_SYMBOL_GPL(tracepoint_probe_unregister(call->tp,\n",
      "\t\t\t\t\t\t call->class->perf_probe,\n",
      "\t\t\t\t\t    call);\n",
      "\t\treturn 0;\n",
      "\tcase '$':\n",
      "\t\tstrcpy(remcom_in_buffer, cmd);\n",
      "\t\treturn 0;\n",
      "\tcase TRACE_REG_PERF_REGISTER:\n",
      "\t\treturn reg_event_syscall_enter(file, event);\n",
      "\t\treturn;\n",
      "\t}\n",
      "\n",
      "retry:\n",
      "\tif (!task_function_call(task, __perf_cgroup_move, task);\n",
      "}\n",
      "\n",
      "static void perf_cgroup_exit(struct cgroup_subsys_state *pos)\n",
      "{\n",
      "\tstruct cgroup_subsys_state *parent_css)\n",
      "{\n",
      "\tstruct freezer *freezer;\n",
      "\n",
      "\tfreezer = kzalloc(sizeof(*data), gfpflags);\n",
      "\tif (!data->cpu_data)\n",
      "\t\tgoto out_err_free;\n",
      "\n",
      "\tfor_each_possible_cpu(cpu)\n",
      "\t\tset_bit(0, &per_cpu(tick_cpu_sched, cpu);\n",
      "\n",
      "# ifdef CONFIG_HIGH_RES_TIMERS\n",
      "\n",
      "/*\n",
      " * High resolution timer enabled ?\n",
      " */\n",
      "static int tick_nohz_init_all(void)\n",
      "{\n",
      "\tint err = device_register(&tick_bc_dev);\n",
      "\n",
      "\tif (!err)\n",
      "\t\terr = register_module_notifier(&ftrace_module_exit_nb);\n",
      "\tif (ret)\n",
      "\t\tpr_warning(\"Failed to register tracepoint module enter notifier\\n\");\n",
      "\n",
      "\treturn ret;\n",
      "}\n",
      "\n",
      "/*\n",
      " * Avoid consuming memory with our now useless rbtree.\n",
      " */\n",
      "static int enqueue_hrtimer(struct hrtimer *timer) { }\n",
      "static inline void clocksource_dequeue_watchdog(struct clocksource *cs) { }\n",
      "static inline void sched_rt_rq_dequeue(rt_rq);\n",
      "\t\t\treturn 1;\n",
      "\t\t}\n",
      "\t}\n",
      "\n",
      "\thlock = curr->held_locks + i;\n",
      "\t\t/*\n",
      "\t\t * We must not cross into another context:\n",
      "\t\t */\n",
      "\t\tif (move_group) {\n",
      "\t\t/*\n",
      "\t\t * Wait for all pre-existing t->on_rq and t->nvcsw\n",
      "\t\t * transitions to complete.  Invoking synchronize_sched_expedited();\n",
      "}\n",
      "EXPORT_SYMBOL_GPL(srcu_init_notifier_head(struct srcu_notifier_head *nh,\n",
      "\t\t\t\t unsigned long flags)\n",
      "{\n",
      "\t__irq_put_desc_unlock(desc, flags);\n",
      "\treturn ret;\n",
      "}\n",
      "\n",
      "#define DEFINE_OUTPUT_COPY(func_name, memcpy_func)\t\t\t\\\n",
      "static inline unsigned int count_highmem_pages(void) { return 0; }\n",
      "static inline int trace_branch_enable(struct trace_uprobe, consumer);\n",
      "\n",
      "\tudd.tu = tu;\n",
      "\tudd.bp_addr = instruction_pointer(regs);\n",
      "\t\tdata = DATAOF_TRACE_ENTRY(entry, false);\n",
      "\t}\n",
      "\n",
      "\tmemcpy(data, ucb->buf, tu->tp.size + dsize) {\n",
      "\t\tint len = tu->tp.size + dsize;\n",
      "\tsize = ALIGN(size + sizeof(u32), sizeof(u64));\n",
      "\tsize -= sizeof(u32);\n",
      "\n",
      "\trec = (struct syscall_trace_enter {\n",
      "\tstruct trace_entry\tent;\n",
      "\tunsigned long\t\t\tip;\n",
      "\tunsigned long\t\t\tcache_read;\n",
      "\tu64\t\t\t\tread_stamp;\n",
      "\t/* ring buffer pages to update, > 0 to add, < 0 to remove */\n",
      "\tint\t\t\t\tnr_pages_to_update;\n",
      "\tstruct list_head list;\n",
      "};\n",
      "\n",
      "struct postfix_elt {\n",
      "\tint op;\n",
      "\tchar *operand;\n",
      "\tstruct list_head *vec;\n",
      "\n",
      "\tif (idx < TVR_SIZE) {\n",
      "\t\tint i = expires & TVR_MASK;\n",
      "\t\tvec = base->tv4.vec + i;\n",
      "\t} else if (idx < 1 << (TVR_BITS + 2 * TVN_BITS)) {\n",
      "\t\tint i = (expires >> (TVR_BITS + 2 * TVN_BITS)) {\n",
      "\t\tint i = (expires >> (TVR_BITS + 2 * TVN_BITS)) {\n",
      "\t\tint i = (expires >> (TVR_BITS + 3 * TVN_BITS)) & TVN_MASK;\n",
      "\t\tvec = base->tv4.vec + i;\n",
      "\t} else if (idx < 1 << (TVR_BITS + TVN_BITS)) & TVN_MASK)\n",
      "\n",
      "/**\n",
      " * __run_timers - run all expired timers (if any) on this CPU.\n",
      " * @base: the timer vector to be processed.\n",
      " *\n",
      " * This function is exported for use by the signal deliver code.  It is\n",
      " * called just prior to the info block being released and passes that\n",
      " * block to us.  It's function is to update the overrun entry AND to\n",
      " * restart the timer.  It should only be called by rtc_resume(), and allows\n",
      " * a suspend offset to be injected into the timekeeping values.\n",
      " */\n",
      "void timekeeping_inject_offset(struct timespec *ts);\n",
      "extern s32 timekeeping_get_tai_offset(void);\n",
      "extern void tick_clock_notify(void)\n",
      "{\n",
      "\tint cpu;\n",
      "\n",
      "\tif (!watchdog_user_enabled)\n",
      "\t\treturn;\n",
      "\n",
      "\tif (cpumask_equal(&p->cpus_allowed, new_mask))\n",
      "\t\tgoto out;\n",
      "\n",
      "\tcpu = smp_processor_id();\n",
      "\n",
      "\t/*\n",
      "\t * Unthrottle events, since we scheduled we might have missed several\n",
      "\t * ticks already, also for a heavily scheduling task there is little\n",
      "\t * guarantee it'll get a tick in a timely manner.\n",
      " * Because an uncertain amount of memory will be freed in some uncertain\n",
      " * timeframe, we do not claim to have freed anything.\n",
      " */\n",
      "static int cpu_hotplug_disabled;\n",
      "\n",
      "#ifdef C\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(lm, length=5000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Order 10 is pretty much junk. In order 15 things sort-of make sense, but we jump abruptly between the `[sic]`\n",
    "and by order 20 we are doing quite nicely -- but are far from keeping good indentation and brackets. \n",
    "\n",
    "How could we? we do not have the memory, and these things are not modeled at all. While we could quite easily enrich our model to support also keeping track of brackets and indentation (by adding information such as \"have I seen ( but not )\" to the conditioning history), this requires extra work, non-trivial human reasoning, and will make the model significantly more complex. \n",
    "\n",
    "Karpathy's LSTM, on the other hand, seemed to have just learn it on its own. And that's impressive.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
